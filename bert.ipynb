{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2617192,"sourceType":"datasetVersion","datasetId":1590810}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-04T06:01:26.421644Z","iopub.execute_input":"2024-01-04T06:01:26.422094Z","iopub.status.idle":"2024-01-04T06:01:26.838730Z","shell.execute_reply.started":"2024-01-04T06:01:26.422059Z","shell.execute_reply":"2024-01-04T06:01:26.837105Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/emotion-dataset/validation.csv\n/kaggle/input/emotion-dataset/training.csv\n/kaggle/input/emotion-dataset/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-12-30T18:31:08.174362Z","iopub.execute_input":"2023-12-30T18:31:08.175375Z","iopub.status.idle":"2023-12-30T18:31:08.180833Z","shell.execute_reply.started":"2023-12-30T18:31:08.175323Z","shell.execute_reply":"2023-12-30T18:31:08.179703Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**BERT EMBEDDINGS**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertModel\nimport pandas as pd\n\n# Load pre-trained BERT model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertModel.from_pretrained(model_name)\n\n# Example dataset (replace this with loading your actual dataset)\ndata = {\n    'text': ['Example sentence 1', 'Another sentence here', 'And so on...'],\n    'label': ['label1', 'label2', 'label3']\n}\ndf = pd.read_csv('/kaggle/input/emotion-dataset/training.csv')\n\n# Tokenize and generate embeddings for each text in the dataset\nembeddings = []\nfor text in df['text']:\n    # Tokenize text\n    encoded_input = tokenizer(text, return_tensors='pt')\n    \n    # Generate embeddings\n    with torch.no_grad():\n        output = model(**encoded_input)\n    \n    # Extract embeddings from BERT's output\n    last_hidden_states = output.last_hidden_state\n    sentence_embedding = torch.mean(last_hidden_states, dim=1).squeeze().numpy()\n    embeddings.append(sentence_embedding)\n\n# Add embeddings to the DataFrame\ndf['embeddings'] = embeddings\n\n# Now df contains the original text, labels, and corresponding BERT embeddings\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:01:29.798545Z","iopub.execute_input":"2024-01-04T06:01:29.799569Z","iopub.status.idle":"2024-01-04T06:09:45.155431Z","shell.execute_reply.started":"2024-01-04T06:01:29.799516Z","shell.execute_reply":"2024-01-04T06:09:45.153337Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9f60deb30e44422aa220c2b02951547"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2b170f853c948689737368f3eb9702d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be86bacb96144a1bae172b023d922f69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b32c39656df436997f750337d9d09fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddf4a7c5e4e54282b73e7c2ea72da0e6"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Extract embeddings from BERT's output\u001b[39;00m\n\u001b[1;32m     28\u001b[0m last_hidden_states \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlast_hidden_state\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pytorch_utils.py:242\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"**SVC on  BERT**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Assuming df contains the embeddings and labels\n\n# Split data into features (embeddings) and labels\nX = df['embeddings'].to_list()  # Features (embeddings)\ny = df['label']  # Labels\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize SVM classifier\nsvm = SVC(kernel='linear', C=1.0, random_state=42)\n\n# Train the SVM classifier\nsvm.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = svm.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T14:28:52.567483Z","iopub.execute_input":"2023-12-30T14:28:52.567839Z","iopub.status.idle":"2023-12-30T14:30:53.148667Z","shell.execute_reply.started":"2023-12-30T14:28:52.567809Z","shell.execute_reply":"2023-12-30T14:30:53.147297Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Accuracy: 0.62\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.62      0.73      0.67       946\n           1       0.68      0.76      0.72      1021\n           2       0.54      0.34      0.42       296\n           3       0.58      0.48      0.53       427\n           4       0.59      0.49      0.53       397\n           5       0.44      0.27      0.34       113\n\n    accuracy                           0.62      3200\n   macro avg       0.57      0.51      0.53      3200\nweighted avg       0.62      0.62      0.61      3200\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**RFC on BERT**","metadata":{}},{"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n#from sklearn.metrics import accuracy_score, classification_report\n\n# Assuming df contains the embeddings and labels\n\n# Split data into features (embeddings) and labels\nX = df['embeddings'].to_list()  # Features (embeddings)\ny = df['label']  # Labels\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize Random Forest classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the Random Forest classifier\nrf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rf.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T14:30:53.151268Z","iopub.execute_input":"2023-12-30T14:30:53.151623Z","iopub.status.idle":"2023-12-30T14:31:34.751711Z","shell.execute_reply.started":"2023-12-30T14:30:53.151592Z","shell.execute_reply":"2023-12-30T14:31:34.750708Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Accuracy: 0.52\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.49      0.75      0.59       946\n           1       0.54      0.87      0.67      1021\n           2       0.00      0.00      0.00       296\n           3       0.67      0.11      0.19       427\n           4       0.50      0.05      0.09       397\n           5       0.00      0.00      0.00       113\n\n    accuracy                           0.52      3200\n   macro avg       0.37      0.30      0.26      3200\nweighted avg       0.47      0.52      0.42      3200\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**roberta embeddings**","metadata":{}},{"cell_type":"code","source":"#import torch\nfrom transformers import RobertaModel, RobertaTokenizer\n#import pandas as pd\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = 'roberta-base'  # You can use different variations of RoBERTa if needed\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaModel.from_pretrained(model_name)\n\n\n# Tokenize and obtain RoBERTa embeddings\ndef get_roberta_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling of token embeddings\n    return embeddings\n\n# Apply the function to your dataset\nembeddings_list = []\nfor row in df['text']:\n    embedding = get_roberta_embeddings(row)\n    embeddings_list.append(embedding)\n\n# Concatenate the embeddings and add them as new columns in your dataset\nembeddings_tensor = torch.cat(embeddings_list)\nembeddings_df = pd.DataFrame(embeddings_tensor.numpy())\n\n# Merge the original dataframe with the embeddings dataframe\nresult_df = pd.concat([df, embeddings_df], axis=1)\nprint(result_df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**roberta emd**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaModel, RobertaTokenizer\nimport pandas as pd\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = 'roberta-base'  # You can use different variations of RoBERTa if needed\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaModel.from_pretrained(model_name)\n\n# Your existing DataFrame with 'text' column\n# Assuming df contains the text data\n# ...\n\n# Tokenize and obtain RoBERTa embeddings\ndef get_roberta_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling of token embeddings\n    return embeddings\n\n# Apply the function to your dataset and store embeddings in a new column 'roberta_emb'\ndf['roberta_emb'] = df['text'].apply(lambda x: get_roberta_embeddings(x)[0].numpy())\n\n# Display the DataFrame with the new 'roberta_emb' column containing embeddings\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-31T09:14:37.411998Z","iopub.execute_input":"2023-12-31T09:14:37.412395Z","iopub.status.idle":"2023-12-31T09:37:43.586666Z","shell.execute_reply.started":"2023-12-31T09:14:37.412360Z","shell.execute_reply":"2023-12-31T09:37:43.585393Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c4620278a3042ab8ac9febd979e1167"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92a391486f95472f886626483484ee30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"528b45127ec341a7a791e5158acf818f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c2744688bf64911a159db8cb6df1841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d16ab5aa63c045028c9f81562e02348b"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"                                                    text  label  \\\n0                                i didnt feel humiliated      0   \n1      i can go from feeling so hopeless to so damned...      0   \n2       im grabbing a minute to post i feel greedy wrong      3   \n3      i am ever feeling nostalgic about the fireplac...      2   \n4                                   i am feeling grouchy      3   \n...                                                  ...    ...   \n15995  i just had a very brief time in the beanbag an...      0   \n15996  i am now turning and i feel pathetic that i am...      0   \n15997                     i feel strong and good overall      1   \n15998  i feel like this was such a rude comment and i...      3   \n15999  i know a lot but i feel so stupid because i ca...      0   \n\n                                              embeddings  \\\n0      [-0.029084232, 0.28403535, -0.15271899, 0.2054...   \n1      [0.062095962, 0.35217127, 0.1729311, -0.079297...   \n2      [0.44961998, 0.075376205, 0.4445088, 0.2008439...   \n3      [-0.2040793, 0.12129233, 0.49728298, -0.293353...   \n4      [-0.267237, 0.40239015, 0.106732294, -0.289782...   \n...                                                  ...   \n15995  [0.036930032, 0.13965842, -0.011358027, -0.157...   \n15996  [0.09091488, 0.3812088, 0.35357383, -0.1902150...   \n15997  [0.16753188, -0.0113387965, 0.1569557, -0.1350...   \n15998  [-0.122115344, 0.25696644, 0.22916913, -0.1250...   \n15999  [0.11180659, 0.3933487, 0.24906981, -0.0286107...   \n\n                                             roberta_emb  \n0      [-0.029875472, -0.12261019, -0.075354286, -0.1...  \n1      [0.027807666, 0.08506984, 0.08304222, -0.19231...  \n2      [-0.018578397, -0.13414888, -0.018878791, -0.1...  \n3      [0.08331484, 0.018418241, -0.020032551, -0.122...  \n4      [0.001202818, 0.06340924, 0.03927246, -0.13768...  \n...                                                  ...  \n15995  [0.042974286, -0.06786098, 0.03850568, -0.0576...  \n15996  [0.062157393, -0.03378462, 0.0077818087, 0.015...  \n15997  [-0.03378327, 0.0037925518, 0.053695656, -0.13...  \n15998  [0.024002088, -0.024004865, -0.011801992, -0.0...  \n15999  [0.081347406, -0.14630297, -0.012619911, -0.09...  \n\n[16000 rows x 4 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**SVC**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Split the data into features (embeddings) and labels\nX = df['roberta_emb'].tolist()  # Features (embeddings)\ny = df['label']  # Labels\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize SVM classifier\nsvm = SVC(kernel='linear', C=1.0, random_state=42)\n\n# Train the SVM classifier\nsvm.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = svm.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T15:10:06.452920Z","iopub.execute_input":"2023-12-30T15:10:06.453290Z","iopub.status.idle":"2023-12-30T15:11:37.497744Z","shell.execute_reply.started":"2023-12-30T15:10:06.453255Z","shell.execute_reply":"2023-12-30T15:11:37.496519Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Accuracy: 0.66\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.65      0.75      0.70       946\n           1       0.70      0.81      0.75      1021\n           2       0.62      0.34      0.44       296\n           3       0.62      0.53      0.57       427\n           4       0.64      0.52      0.57       397\n           5       0.53      0.35      0.42       113\n\n    accuracy                           0.66      3200\n   macro avg       0.63      0.55      0.58      3200\nweighted avg       0.65      0.66      0.65      3200\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**RFC**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Split the data into features (embeddings) and labels\nX = df['roberta_emb'].tolist()  # Features (embeddings)\ny = df['label']  # Labels\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize Random Forest classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the Random Forest classifier\nrf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rf.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T15:11:37.499182Z","iopub.execute_input":"2023-12-30T15:11:37.499672Z","iopub.status.idle":"2023-12-30T15:12:19.435286Z","shell.execute_reply.started":"2023-12-30T15:11:37.499636Z","shell.execute_reply":"2023-12-30T15:12:19.434036Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Accuracy: 0.51\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.48      0.74      0.58       946\n           1       0.52      0.86      0.65      1021\n           2       0.00      0.00      0.00       296\n           3       0.68      0.07      0.12       427\n           4       0.85      0.06      0.10       397\n           5       0.00      0.00      0.00       113\n\n    accuracy                           0.51      3200\n   macro avg       0.42      0.29      0.24      3200\nweighted avg       0.50      0.51      0.41      3200\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**ANN on roberta**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Convert labels to numerical values using LabelEncoder\nlabel_encoder = LabelEncoder()\ndf['encoded_labels'] = label_encoder.fit_transform(df['label'])\n\n# Split the data into features (embeddings) and encoded labels\nX = df['roberta_emb'].tolist()  # Features (embeddings)\ny = df['encoded_labels']  # Encoded Labels\n\n# Convert to PyTorch tensors\nX = torch.tensor(X)\ny = torch.tensor(y)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the neural network architecture\nclass ANN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ANN, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\n# Set input, hidden, and output sizes\ninput_size = len(X[0])\nhidden_size = 128  # Adjust the hidden layer size as needed\noutput_size = len(label_encoder.classes_)\n\n# Initialize the neural network model\nmodel = ANN(input_size, hidden_size, output_size)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n\n# Training the model\nnum_epochs = 300  # Adjust the number of epochs as needed\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(X_train.float())\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n    if (epoch+1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Evaluation on test data\nwith torch.no_grad():\n    model.eval()\n    outputs = model(X_test.float())\n    _, predicted = torch.max(outputs, 1)\n    accuracy = accuracy_score(y_test, predicted)\n    print(f'Accuracy: {accuracy:.2f}')\n    print('Classification Report:')\n    print(classification_report(y_test, predicted))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T15:12:19.436716Z","iopub.execute_input":"2023-12-30T15:12:19.437059Z","iopub.status.idle":"2023-12-30T15:12:35.914285Z","shell.execute_reply.started":"2023-12-30T15:12:19.437023Z","shell.execute_reply":"2023-12-30T15:12:35.913531Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch [10/300], Loss: 1.5469\nEpoch [20/300], Loss: 1.4947\nEpoch [30/300], Loss: 1.4298\nEpoch [40/300], Loss: 1.3619\nEpoch [50/300], Loss: 1.2945\nEpoch [60/300], Loss: 1.2341\nEpoch [70/300], Loss: 1.1827\nEpoch [80/300], Loss: 1.1385\nEpoch [90/300], Loss: 1.1000\nEpoch [100/300], Loss: 1.0663\nEpoch [110/300], Loss: 1.0368\nEpoch [120/300], Loss: 1.0108\nEpoch [130/300], Loss: 0.9879\nEpoch [140/300], Loss: 0.9675\nEpoch [150/300], Loss: 0.9492\nEpoch [160/300], Loss: 0.9328\nEpoch [170/300], Loss: 0.9180\nEpoch [180/300], Loss: 0.9048\nEpoch [190/300], Loss: 0.8928\nEpoch [200/300], Loss: 0.8820\nEpoch [210/300], Loss: 0.8721\nEpoch [220/300], Loss: 0.8629\nEpoch [230/300], Loss: 0.8544\nEpoch [240/300], Loss: 0.8464\nEpoch [250/300], Loss: 0.8389\nEpoch [260/300], Loss: 0.8317\nEpoch [270/300], Loss: 0.8249\nEpoch [280/300], Loss: 0.8184\nEpoch [290/300], Loss: 0.8121\nEpoch [300/300], Loss: 0.8060\nAccuracy: 0.65\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.66      0.74      0.70       946\n           1       0.69      0.81      0.75      1021\n           2       0.58      0.32      0.41       296\n           3       0.61      0.52      0.56       427\n           4       0.61      0.53      0.57       397\n           5       0.52      0.30      0.38       113\n\n    accuracy                           0.65      3200\n   macro avg       0.61      0.54      0.56      3200\nweighted avg       0.64      0.65      0.64      3200\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**ANN on bert**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Convert labels to numerical values using LabelEncoder\nlabel_encoder = LabelEncoder()\ndf['encoded_labels'] = label_encoder.fit_transform(df['label'])\n\n# Split the data into features (embeddings) and encoded labels\nX = df['embeddings'].tolist()  # Features (embeddings)\ny = df['encoded_labels']  # Encoded Labels\n\n# Convert to PyTorch tensors\nX = torch.tensor(X)\ny = torch.tensor(y)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the neural network architecture\nclass ANN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ANN, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\n# Set input, hidden, and output sizes\ninput_size = len(X[0])\nhidden_size = 128  # Adjust the hidden layer size as needed\noutput_size = len(label_encoder.classes_)\n\n# Initialize the neural network model\nmodel = ANN(input_size, hidden_size, output_size)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n\n# Training the model\nnum_epochs = 300  # Adjust the number of epochs as needed\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(X_train.float())\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n    if (epoch+1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Evaluation on test data\nwith torch.no_grad():\n    model.eval()\n    outputs = model(X_test.float())\n    _, predicted = torch.max(outputs, 1)\n    accuracy = accuracy_score(y_test, predicted)\n    print(f'Accuracy: {accuracy:.2f}')\n    print('Classification Report:')\n    print(classification_report(y_test, predicted))","metadata":{"execution":{"iopub.status.busy":"2023-12-30T15:12:35.915162Z","iopub.execute_input":"2023-12-30T15:12:35.915456Z","iopub.status.idle":"2023-12-30T15:12:52.887654Z","shell.execute_reply.started":"2023-12-30T15:12:35.915430Z","shell.execute_reply":"2023-12-30T15:12:52.886572Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch [10/300], Loss: 1.4214\nEpoch [20/300], Loss: 1.2776\nEpoch [30/300], Loss: 1.1884\nEpoch [40/300], Loss: 1.1268\nEpoch [50/300], Loss: 1.0794\nEpoch [60/300], Loss: 1.0416\nEpoch [70/300], Loss: 1.0099\nEpoch [80/300], Loss: 0.9825\nEpoch [90/300], Loss: 0.9588\nEpoch [100/300], Loss: 0.9383\nEpoch [110/300], Loss: 0.9201\nEpoch [120/300], Loss: 0.9038\nEpoch [130/300], Loss: 0.8887\nEpoch [140/300], Loss: 0.8746\nEpoch [150/300], Loss: 0.8613\nEpoch [160/300], Loss: 0.8486\nEpoch [170/300], Loss: 0.8366\nEpoch [180/300], Loss: 0.8250\nEpoch [190/300], Loss: 0.8139\nEpoch [200/300], Loss: 0.8030\nEpoch [210/300], Loss: 0.7926\nEpoch [220/300], Loss: 0.7826\nEpoch [230/300], Loss: 0.7727\nEpoch [240/300], Loss: 0.7631\nEpoch [250/300], Loss: 0.7534\nEpoch [260/300], Loss: 0.7436\nEpoch [270/300], Loss: 0.7340\nEpoch [280/300], Loss: 0.7248\nEpoch [290/300], Loss: 0.7156\nEpoch [300/300], Loss: 0.7065\nAccuracy: 0.64\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.65      0.70      0.68       946\n           1       0.68      0.80      0.74      1021\n           2       0.52      0.30      0.38       296\n           3       0.59      0.53      0.56       427\n           4       0.58      0.54      0.56       397\n           5       0.45      0.26      0.33       113\n\n    accuracy                           0.64      3200\n   macro avg       0.58      0.52      0.54      3200\nweighted avg       0.63      0.64      0.63      3200\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**classification using bert**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\ndf=pd.read_csv('/kaggle/input/emotion-dataset/training.csv')\n\n# Load pre-trained BERT model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=6)  # num_classes is the number of unique classes in your dataset\n\n# Assuming df contains the BERT embeddings and labels\nX = df['text'].tolist()  # Text data\ny = df['label']  # Target\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Tokenize text after splitting the data\ntokenized_train = tokenizer(X_train, padding=True, truncation=True, return_tensors='pt')\ntokenized_test = tokenizer(X_test, padding=True, truncation=True, return_tensors='pt')\n\n# Convert y_train to tensor\ny_train = torch.tensor(y_train.values.astype(np.int64))  # Convert y_train to a tensor\n\n\n# Create data loaders\ntrain_data = torch.utils.data.TensorDataset(tokenized_train['input_ids'], tokenized_train['attention_mask'], torch.tensor(y_train))\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=20)\n\n# Define training parameters\n#batch_size = 16\nepochs = 3\nlearning_rate = 2e-5\n\n\n\n# Set optimizer and loss function\noptimizer = AdamW(model.parameters(), lr=learning_rate)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Training loop\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    \n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_ids, attention_mask, labels = batch\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n    \n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}/{epochs} - Average Training Loss: {avg_train_loss:.4f}\")\n\n# Evaluate the model\nmodel.eval()\ntest_input_ids = X_test['input_ids']\ntest_attention_mask = X_test['attention_mask']\nwith torch.no_grad():\n    outputs = model(test_input_ids, attention_mask=test_attention_mask)\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=1)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T14:58:18.022950Z","iopub.execute_input":"2024-01-04T14:58:18.023317Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_42/1438899797.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  train_data = torch.utils.data.TensorDataset(tokenized_train['input_ids'], tokenized_train['attention_mask'], torch.tensor(y_train))\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Average Training Loss: 0.5725\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**distilbert**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset\ndf = pd.read_csv('/kaggle/input/emotion-dataset/training.csv')  \n\n# Display the first few rows to check the data\n\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizerFast\n\n# Split the data into training and validation sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    df['text'], df['label'], test_size=0.2, random_state=42\n)\n\n# Initialize DistilBERT tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n# Tokenize the text data\ntrain_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\ntest_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\nfrom sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Fit and transform labels for training data\ntrain_labels_encoded = label_encoder.fit_transform(train_labels)\n\n# Transform labels for validation data\ntest_labels_encoded = label_encoder.transform(test_labels)\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Create PyTorch Datasets\ntrain_dataset = CustomDataset(train_encodings, train_labels_encoded)\ntest_dataset = CustomDataset(test_encodings, test_labels_encoded)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\nfrom transformers import DistilBertForSequenceClassification, AdamW\nfrom tqdm import tqdm\n\n# Initialize the model\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=6)\n\n# Define optimizer and learning rate scheduler\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training loop\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.train()\n\nfor epoch in range(3):  # Set your desired number of epochs\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n# Evaluation\nmodel.eval()\ntest_preds = []\ntest_true = []\n\nfor batch in tqdm(test_loader):\n    with torch.no_grad():\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        test_preds.extend(preds.cpu().detach().numpy())\n        test_true.extend(labels.cpu().detach().numpy())\n\n# Calculate accuracy\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(test_true, test_preds)\nprint(f\"test Accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T14:45:18.220641Z","iopub.execute_input":"2024-01-04T14:45:18.220980Z","iopub.status.idle":"2024-01-04T14:50:19.295178Z","shell.execute_reply.started":"2024-01-04T14:45:18.220955Z","shell.execute_reply":"2024-01-04T14:50:19.294225Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1: 100%|██████████| 800/800 [01:37<00:00,  8.18it/s]\nEpoch 2: 100%|██████████| 800/800 [01:37<00:00,  8.20it/s]\nEpoch 3: 100%|██████████| 800/800 [01:37<00:00,  8.20it/s]\n100%|██████████| 200/200 [00:06<00:00, 30.69it/s]","output_type":"stream"},{"name":"stdout","text":"test Accuracy: 0.9325\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}